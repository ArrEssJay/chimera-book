\chapter{Convolutional Codes \& Viterbi Decoding}
\label{ch:convolutional-codes}

\begin{nontechnical}
\textbf{Convolutional codes + Viterbi decoding is like having a GPS that considers your entire journey to figure out where you really are---even if some GPS samples are noisy!}


\textbf{The Problem:} 
\begin{itemize}
\item Noise corrupts transmitted bits: some 0s become 1s, some 1s become 0s
\item How do you figure out what was actually sent?
\end{itemize}

\textbf{The Convolutional Code Solution---Add memory:}
\begin{enumerate}
\item Instead of encoding each bit independently, the encoder ``remembers'' previous bits
\item Each output bit depends on current + past few input bits
\item This creates patterns---if one bit gets corrupted, the pattern breaks and decoder notices!
\end{enumerate}

\textbf{The GPS Analogy:}
\begin{itemize}
\item \textbf{Bad GPS:} Each position reading is independent. Get noisy reading? Can't tell if it's wrong!
\item \textbf{Smart GPS:} Considers your speed, direction, previous positions. Get noisy reading that says you teleported 5 miles? ``That's impossible, ignore it!''
\end{itemize}

\textbf{Viterbi Decoding---Find the most likely path:}
\begin{itemize}
\item Looks at entire received sequence
\item Considers all possible paths the data could have taken
\item Picks the path that best matches what was received (even with errors!)
\end{itemize}

\textbf{Real-world example---Space probes:}
\begin{itemize}
\item \textbf{Voyager spacecraft:} 15+ billion miles away, incredibly noisy signal
\item Uses convolutional code with Viterbi decoding
\item Can correct errors even when 30--40\% of bits are corrupted!
\item This is why we still get photos from interstellar space!
\end{itemize}

\textbf{Everyday uses:}
\begin{itemize}
\item \textbf{WiFi:} 802.11a/g use convolutional codes
\item \textbf{Satellite TV:} DVB-S uses convolutional + Viterbi
\item \textbf{GSM (2G):} Your old cell phone used this
\item \textbf{GPS signals:} Navigation satellites use convolutional codes
\end{itemize}

\textbf{Why it works:}
\begin{itemize}
\item Pattern-based: Errors break patterns, decoder spots them
\item Context-aware: Uses past data to correct current data
\item Optimal: Viterbi finds the single best answer (mathematically proven!)
\end{itemize}

\textbf{Trade-off:} More memory = better error correction BUT more complex decoder. Most systems use 3--7 bits of memory (called ``constraint length'').

\textbf{Fun fact:} Andrew Viterbi invented this algorithm in 1967 for deep space communications---then co-founded Qualcomm, making billions from the algorithm used in every cell phone!
\end{nontechnical}

\section{Overview}

\textbf{Convolutional codes} are a class of error-correcting codes that encode data \textbf{continuously} rather than in fixed blocks. Unlike block codes that treat each codeword independently, convolutional codes maintain \textbf{memory} of previous input bits, enabling powerful error correction through pattern-based decoding.

\begin{keyconcept}
The fundamental distinction between block codes and convolutional codes lies in \textbf{memory}:
\begin{itemize}
\item \textbf{Block codes:} Encode $k$ bits $\rightarrow$ $n$ bits independently (memoryless)
\item \textbf{Convolutional codes:} Each output depends on current + \textbf{previous} input bits (memory)
\end{itemize}

This memory creates \textbf{structural dependencies} that enable the Viterbi algorithm to perform optimal maximum-likelihood (ML) decoding efficiently.
\end{keyconcept}

\textbf{Applications:} Convolutional codes are ubiquitous in modern communications:
\begin{itemize}
\item \textbf{Satellite:} DVB-S, GPS navigation signals
\item \textbf{Wireless:} WiFi (802.11a/g), LTE control channels
\item \textbf{Deep space:} Voyager spacecraft, Mars rovers
\item \textbf{Mobile:} GSM (2G), early 3G systems
\end{itemize}

\textbf{Key advantages:}
\begin{itemize}
\item Excellent performance with soft-decision decoding
\item Low latency (suitable for streaming applications)
\item Optimal ML decoding via Viterbi algorithm
\item Easily punctured to achieve variable code rates
\end{itemize}

\section{Fundamental Parameters}

\subsection{Constraint Length ($K$)}

The \textbf{constraint length} $K$ defines the number of input bits that influence each output bit:
\begin{equation}
K = \text{number of input bits affecting output}
\label{eq:constraint-length}
\end{equation}

The encoder maintains \textbf{memory} through a shift register with:
\begin{equation}
m = K - 1 \quad \text{(shift register stages)}
\label{eq:memory-order}
\end{equation}
where:
\begin{itemize}
\item $K$ = constraint length (total influencing bits)
\item $m$ = memory order (number of previous bits stored)
\end{itemize}

\textbf{Example:} For $K = 3$, the current bit plus 2 previous bits (3 total) determine each output.

The number of encoder states is:
\begin{equation}
N_{\text{states}} = 2^{K-1} = 2^m
\label{eq:num-states}
\end{equation}

\subsection{Code Rate ($r$)}

The \textbf{code rate} quantifies redundancy:
\begin{equation}
r = \frac{k}{n}
\label{eq:code-rate}
\end{equation}
where:
\begin{itemize}
\item $k$ = number of input bits per time step
\item $n$ = number of output bits per time step
\item $r$ = code rate (fraction of information bits)
\end{itemize}

\textbf{Common code rates:}
\begin{itemize}
\item $r = 1/2$: 1 input bit $\rightarrow$ 2 output bits (strong coding, deep space)
\item $r = 1/3$: 1 input bit $\rightarrow$ 3 output bits (very strong coding, low SNR)
\item $r = 2/3$: 2 input bits $\rightarrow$ 3 output bits (punctured from $r = 1/2$)
\item $r = 3/4$: 3 input bits $\rightarrow$ 4 output bits (high throughput)
\end{itemize}

Lower code rates provide more redundancy and stronger error correction at the cost of reduced data throughput.

\section{Encoder Structure}

A convolutional encoder consists of:
\begin{enumerate}
\item \textbf{Shift register:} Stores $K-1$ previous input bits
\item \textbf{Modulo-2 adders (XOR gates):} Combine register contents
\item \textbf{Generator polynomials:} Define connections from register to XORs
\end{enumerate}

\subsection{Generic Encoder Architecture}

\begin{center}
\begin{tikzpicture}[
  block/.style={rectangle, draw, minimum width=1cm, minimum height=0.8cm, font=\sffamily\small},
  node distance=1.5cm,
  font=\small
]

% Shift register stages
\node[block] (d0) {$D_0$};
\node[block, right of=d0] (d1) {$D_1$};
\node[block, right of=d1] (d2) {$D_2$};

% Input
\node[left of=d0, node distance=2cm] (input) {\sffamily Input};
\draw[->,thick] (input) -- node[above,font=\scriptsize] {$u_k$} (d0);

% Connections between stages
\draw[->,thick] (d0) -- (d1);
\draw[->,thick] (d1) -- (d2);

% XOR gates for output 1
\node[below of=d1, node distance=2cm, font=\small] (xor1) {$\bigoplus$};
\node[circle, draw, inner sep=0pt, minimum size=6pt, fill=black] at ($(d0.south)+(0,-0.3)$) (tap1a) {};
\node[circle, draw, inner sep=0pt, minimum size=6pt, fill=black] at ($(d1.south)+(0,-0.3)$) (tap1b) {};
\node[circle, draw, inner sep=0pt, minimum size=6pt, fill=black] at ($(d2.south)+(0,-0.3)$) (tap1c) {};
\draw[-] (d0.south) -- (tap1a);
\draw[-] (d1.south) -- (tap1b);
\draw[-] (d2.south) -- (tap1c);
\draw[-] (tap1a) |- (xor1);
\draw[-] (tap1b) -- (xor1);
\draw[-] (tap1c) |- (xor1);

% Output 1
\node[below of=xor1, node distance=0.8cm] (out1) {\sffamily $v_1$};
\draw[->,thick] (xor1) -- (out1);

% XOR gates for output 2
\node[below of=d0, node distance=3.5cm, font=\small] (xor2) {$\bigoplus$};
\node[circle, draw, inner sep=0pt, minimum size=6pt, fill=black] at ($(d0.south)+(0,-2.2)$) (tap2a) {};
\node[circle, draw, inner sep=0pt, minimum size=6pt, fill=black] at ($(d2.south)+(0,-2.2)$) (tap2c) {};
\draw[-] (tap1a) -- (tap2a);
\draw[-] (tap1c) -- (tap2c);
\draw[-] (tap2a) |- (xor2);
\draw[-] (tap2c) |- (xor2);

% Output 2
\node[below of=xor2, node distance=0.8cm] (out2) {\sffamily $v_2$};
\draw[->,thick] (xor2) -- (out2);

% Labels
\node[above=3pt of d0, font=\scriptsize] {Current};
\node[above=3pt of d1, font=\scriptsize] {Previous};
\node[above=3pt of d2, font=\scriptsize] {Previous};

% Title
\node[above=15pt of d1, font=\sffamily\bfseries] {Rate $r=1/2$, Constraint Length $K=3$};

\end{tikzpicture}
\end{center}

\textbf{Generator polynomials} define the connections:
\begin{itemize}
\item $g_1 = 111_2$ (binary): All three stages connected to XOR$_1$ $\rightarrow$ Output $v_1$
\item $g_2 = 101_2$ (binary): Stages $D_0$ and $D_2$ connected to XOR$_2$ $\rightarrow$ Output $v_2$
\end{itemize}

The generator polynomial determines which shift register taps feed each modulo-2 adder.

\section{Industry Standard: NASA Code}

\subsection{NASA Standard ($r=1/2$, $K=7$)}

The \textbf{NASA standard convolutional code} is one of the most widely deployed error-correcting codes, used in deep-space missions including Voyager, Cassini, and Mars rovers. This code achieves excellent performance with manageable complexity.

\textbf{Parameters:}
\begin{itemize}
\item \textbf{Code rate:} $r = 1/2$ (1 input bit $\rightarrow$ 2 output bits)
\item \textbf{Constraint length:} $K = 7$ (7-bit influence span)
\item \textbf{Memory:} $m = 6$ (6 shift register stages)
\item \textbf{Number of states:} $2^6 = 64$ states
\end{itemize}

\textbf{Generator polynomials} (octal notation):
\begin{align}
g_1 &= 171_8 = 1111001_2 \label{eq:nasa-g1} \\
g_2 &= 133_8 = 1011011_2 \label{eq:nasa-g2}
\end{align}
where:
\begin{itemize}
\item $g_1$ = generator for output $Y_1$ (stages 0,1,2,3,6 connected)
\item $g_2$ = generator for output $Y_2$ (stages 0,2,3,5,6 connected)
\end{itemize}

\begin{calloutbox}{Octal to Binary Conversion}
The octal notation is standard in convolutional code specifications:
\begin{itemize}
\item $171_8 = 001\ 111\ 001_2 = 1111001_2$ (trim leading zeros)
\item $133_8 = 001\ 011\ 011_2 = 1011011_2$
\end{itemize}

Each octal digit (0--7) represents exactly 3 binary bits, making it convenient for generator polynomial specification.
\end{calloutbox}

\subsection{Encoding Example (Simplified $K=3$ Code)}

To illustrate the encoding process, consider a simpler $K=3$ code with generators $g_1 = 111_2$ and $g_2 = 101_2$.

\textbf{Input data:} $101_2$ (3 bits)

\textbf{Initial state:} All zeros $[00]$ (2-stage shift register for $K=3$)

\begin{center}
\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{Time} & \textbf{Input} & \textbf{State} & \textbf{$Y_1$} & \textbf{$Y_2$} & \textbf{Output} \\
\midrule
0 & 1 & 10 & 1 & 1 & 11 \\
1 & 0 & 01 & 1 & 0 & 10 \\
2 & 1 & 10 & 0 & 1 & 01 \\
3 (flush) & 0 & 01 & 1 & 0 & 10 \\
4 (flush) & 0 & 00 & 1 & 1 & 11 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Encoded output:} $11\ 10\ 01\ 10\ 11$ (10 bits for 3 input bits + 2 flush bits)

\textbf{Code rate achieved:} $r = 3/(3+2) \times 2 = 3/10 = 0.3$ (effectively, including flush overhead)

\begin{warningbox}
\textbf{Termination (flushing) is critical.} The encoder must be returned to the all-zeros state by appending $K-1$ zero bits. This ensures the Viterbi decoder can reliably identify the final state, preventing decoding errors at the end of the sequence.

For $K=7$ codes, 6 flush bits are required. For short messages, this overhead can be significant.
\end{warningbox}

\subsection{State Diagram}\label{state-diagram}

\textbf{States}: All possible shift register contents

\textbf{For K=3}: \(2^{K-1} = 2^2 = 4\) states - State 00, State 01,
State 10, State 11

\textbf{Transitions}: Input bit determines next state

\textbf{Example (r=1/2, K=3, g1=111, g2=101)}:

\begin{verbatim}
State diagram:

   00 --0/00--> 00
    |  --1/11--> 10
    
   01 --0/11--> 00
    |  --1/00--> 10
    
   10 --0/10--> 01
    |  --1/01--> 11
    
   11 --0/01--> 01
    |  --1/10--> 11
\end{verbatim}

\textbf{Notation}: Input/Output (e.g., ``1/11'' = input 1 produces
output 11)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Trellis Diagram}

The \textbf{trellis diagram} is the state diagram \textbf{unrolled in time}, showing all possible encoder state transitions over multiple time steps. It is the fundamental structure for Viterbi decoding.

\subsection{Trellis Structure ($K=3$ Example)}

\begin{center}
\begin{tikzpicture}[
  scale=1.2,
  every node/.style={font=\small},
  state/.style={circle, draw, minimum size=6pt, inner sep=1pt, fill=white},
  solid/.style={thick, ->,>=stealth},
  dashed/.style={thick, ->,>=stealth, dashed}
]

% Time labels
\foreach \t in {0,1,2,3,4} {
  \node[font=\scriptsize] at (\t*2.5, 3.5) {$t=\t$};
}

% State labels
\node[font=\scriptsize, left] at (-0.3, 3) {00};
\node[font=\scriptsize, left] at (-0.3, 2) {01};
\node[font=\scriptsize, left] at (-0.3, 1) {10};
\node[font=\scriptsize, left] at (-0.3, 0) {11};

% Draw states at each time
\foreach \t in {0,1,2,3,4} {
  \foreach \s in {0,1,2,3} {
    \node[state] (s\s t\t) at (\t*2.5, 3-\s) {};
  }
}

% Draw transitions (input 0 = solid, input 1 = dashed)
% From state 00
\foreach \t in {0,1,2,3} {
  \pgfmathtruncatemacro{\tnext}{\t+1}
  \draw[solid] (s0t\t) -- (s0t\tnext) node[midway,above,font=\tiny] {0/00};
  \draw[dashed] (s0t\t) -- (s2t\tnext) node[midway,below,font=\tiny] {1/11};
}

% From state 01
\foreach \t in {0,1,2,3} {
  \pgfmathtruncatemacro{\tnext}{\t+1}
  \draw[solid] (s1t\t) -- (s0t\tnext) node[midway,above,font=\tiny] {0/11};
  \draw[dashed] (s1t\t) -- (s2t\tnext) node[midway,below,font=\tiny] {1/00};
}

% From state 10
\foreach \t in {0,1,2,3} {
  \pgfmathtruncatemacro{\tnext}{\t+1}
  \draw[solid] (s2t\t) -- (s1t\tnext) node[midway,above,font=\tiny] {0/10};
  \draw[dashed] (s2t\t) -- (s3t\tnext) node[midway,below,font=\tiny] {1/01};
}

% From state 11
\foreach \t in {0,1,2,3} {
  \pgfmathtruncatemacro{\tnext}{\t+1}
  \draw[solid] (s3t\t) -- (s1t\tnext) node[midway,above,font=\tiny] {0/01};
  \draw[dashed] (s3t\t) -- (s3t\tnext) node[midway,below,font=\tiny] {1/10};
}

% Legend
\node[font=\scriptsize, align=left] at (5, -0.8) {
  \tikz{\draw[solid] (0,0) -- (0.4,0);} Input = 0\\
  \tikz{\draw[dashed] (0,0) -- (0.4,0);} Input = 1\\
  Labels: input/output
};

\end{tikzpicture}
\end{center}

\textbf{Key observations:}
\begin{itemize}
\item Each \textbf{state node} represents the shift register contents at time $t$
\item Each \textbf{branch} represents a state transition caused by an input bit
\item \textbf{Branch labels} show input/output (e.g., ``1/11'' = input 1 produces output 11)
\item A \textbf{path through the trellis} represents a complete encoded sequence
\item The encoder must start at state 00 (after reset/flush)
\end{itemize}

\textbf{Decoding problem:} Given received sequence $r_1, r_2, \ldots, r_N$, find the \textbf{most likely path} through the trellis. The Viterbi algorithm solves this optimally.

\section{Viterbi Algorithm}

The \textbf{Viterbi algorithm}, invented by Andrew J. Viterbi in 1967, performs \textbf{optimal maximum-likelihood (ML) decoding} for convolutional codes. It efficiently searches the exponentially large space of possible paths through the trellis.

\begin{keyconcept}
The Viterbi algorithm exploits the \textbf{Markov property} of convolutional codes: the optimal path to any state at time $t+1$ must include the optimal path to some state at time $t$.

This enables \textbf{dynamic programming}: instead of examining all $2^L$ possible sequences, we only track $2^{K-1}$ survivor paths at each time step.
\end{keyconcept}

\subsection{Algorithm Complexity}

\textbf{Computational complexity:}
\begin{equation}
\mathcal{O}(2^{K-1} \cdot L \cdot n)
\label{eq:viterbi-complexity}
\end{equation}
where:
\begin{itemize}
\item $K$ = constraint length
\item $L$ = sequence length (number of time steps)
\item $n$ = number of outputs per time step (code rate denominator)
\end{itemize}

\textbf{Practical limits:} The algorithm is efficient for $K \leq 9$. Beyond $K=10$, the exponential state space becomes prohibitive ($2^9 = 512$ states vs. $2^{10} = 1024$ states).

\subsection{Core Principle}

At each time step $t$, for each possible state $s$:
\begin{enumerate}
\item Compute \textbf{branch metrics} for all incoming transitions
\item Select the \textbf{survivor path} with minimum cumulative metric
\item Store the survivor path and its metric
\end{enumerate}

After processing all received symbols, \textbf{traceback} from the best final state to recover the decoded sequence.

\subsection{Complete Algorithm Steps}

\textbf{Step 1: Initialization}
\begin{itemize}
\item Set $\text{PM}_0(00) = 0$ (start at all-zeros state)
\item Set $\text{PM}_0(s) = \infty$ for all other states $s \neq 00$
\item Initialize survivor path storage
\end{itemize}

\textbf{Step 2: Forward Pass (for $t = 1$ to $L$)}

For each time step $t$:
\begin{enumerate}
\item \textbf{For each state $s$:}
   \begin{enumerate}
   \item Identify all valid predecessor states $s'$
   \item Compute branch metrics $\text{BM}_t(s' \to s)$
   \item Compute candidate path metrics: $\text{PM}_{t-1}(s') + \text{BM}_t(s' \to s)$
   \item Select minimum: $\text{PM}_t(s) = \min_{s'} [\text{PM}_{t-1}(s') + \text{BM}_t(s' \to s)]$
   \item Store survivor path: record which $s'$ achieved the minimum
   \end{enumerate}
\end{enumerate}

\textbf{Step 3: Termination}
\begin{itemize}
\item If encoder was flushed: select state 00 at time $L$
\item If not flushed: select state with minimum $\text{PM}_L(s)$
\end{itemize}

\textbf{Step 4: Traceback}
\begin{enumerate}
\item Start at selected final state $s_L$
\item Follow survivor paths backward: $s_L \to s_{L-1} \to \cdots \to s_1 \to s_0$
\item Extract input bits that caused each state transition
\item Reverse the sequence to obtain decoded bits
\end{enumerate}

\textbf{Step 5: Output}

Return decoded bit sequence (excluding flush bits if applicable).

\subsection{Branch Metrics}

The \textbf{branch metric} quantifies the ``distance'' between the received symbol and the expected output for a particular state transition.

\subsubsection{Hard-Decision Decoding (Hamming Distance)}

For binary demodulator output ($r_i \in \{0, 1\}$):
\begin{equation}
\text{BM}_{\text{hard}}(s' \to s) = \sum_{i=1}^{n} (r_i \oplus c_i)
\label{eq:branch-metric-hard}
\end{equation}
where:
\begin{itemize}
\item $r_i$ = received bit (0 or 1)
\item $c_i$ = expected output bit for transition $s' \to s$
\item $\oplus$ = modulo-2 addition (XOR)
\item $n$ = number of output bits per symbol
\end{itemize}

This counts the number of bit errors (\textbf{Hamming distance}).

\subsubsection{Soft-Decision Decoding (Euclidean Distance)}

For analog demodulator output ($r_i \in \mathbb{R}$):
\begin{equation}
\text{BM}_{\text{soft}}(s' \to s) = \sum_{i=1}^{n} (r_i - \hat{c}_i)^2
\label{eq:branch-metric-soft}
\end{equation}
where:
\begin{itemize}
\item $r_i \in \mathbb{R}$ = soft received value (e.g., $+3.2$ or $-1.7$)
\item $\hat{c}_i \in \{+1, -1\}$ = expected output mapped to antipodal signals
\end{itemize}

For BPSK modulation: $\hat{c}_i = 2c_i - 1$ maps $\{0,1\}$ to $\{-1,+1\}$.

\begin{keyconcept}
\textbf{Soft-decision decoding provides approximately 2--3 dB coding gain} over hard-decision at the same BER. This is ``free'' performance improvement requiring only analog values from the demodulator instead of binary decisions.

The key insight: preserving \textbf{reliability information} (how confident the demodulator is) enables better decoding decisions.
\end{keyconcept}

\subsection{Path Metric and Survivor Selection}

The \textbf{path metric} is the cumulative distance from the starting state to state $s$ at time $t$:
\begin{equation}
\text{PM}_t(s) = \min_{s'} \left[ \text{PM}_{t-1}(s') + \text{BM}_t(s' \to s) \right]
\label{eq:path-metric}
\end{equation}
where:
\begin{itemize}
\item $\text{PM}_t(s)$ = cumulative path metric to state $s$ at time $t$
\item $\text{PM}_{t-1}(s')$ = path metric to previous state $s'$ at time $t-1$
\item $\text{BM}_t(s' \to s)$ = branch metric for transition $s' \to s$ at time $t$
\item $\min_{s'}$ = minimize over all valid predecessor states $s'$
\end{itemize}

The \textbf{survivor path} to state $s$ is the path achieving the minimum in Equation \ref{eq:path-metric}.

\textbf{Add-Compare-Select (ACS) operation:}
\begin{enumerate}
\item \textbf{Add:} Compute $\text{PM}_{t-1}(s') + \text{BM}_t(s' \to s)$ for each predecessor
\item \textbf{Compare:} Find the minimum among all candidates
\item \textbf{Select:} Choose the survivor path and update $\text{PM}_t(s)$
\end{enumerate}

This ACS operation is the computational core of the Viterbi algorithm, performed $2^{K-1}$ times per time step (once for each state).

\subsubsection{Example (Hard-Decision)}\label{example-hard-decision}

\textbf{Code}: r=1/2, K=3 (4 states)

\textbf{Received}: 11 10 01 11 00

\textbf{Assume}: Start state 00, end state 00

\textbf{Time 0}: Initialize all states (PM = \$\textbackslash infty\$
except state 00)

\textbf{Time 1}: Input unknown, received 11 - Branch
00\$\textbackslash rightarrow\$00 (output 00): Hamming distance = 2 -
Branch 00\$\textbackslash rightarrow\$10 (output 11): Hamming distance =
0 - Update: PM(00) = 2, PM(10) = 0

\textbf{Continue} for all time steps\textbackslash ldots\{\}

\textbf{Final}: Traceback from state with minimum PM

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Free Distance and Error Correction Capability}

\subsection{Definition of Free Distance}

The \textbf{free distance} $d_{\text{free}}$ is the minimum Hamming distance between \textbf{any two distinct encoded sequences} that diverge from and later remerge to the same state in the trellis.

\begin{equation}
d_{\text{free}} = \min_{i \neq j} d_H(c^{(i)}, c^{(j)})
\label{eq:free-distance}
\end{equation}
where:
\begin{itemize}
\item $c^{(i)}$ and $c^{(j)}$ are any two distinct codeword sequences
\item $d_H(\cdot, \cdot)$ is the Hamming distance
\end{itemize}

\subsection{Error Correction Capability}

The free distance determines the \textbf{guaranteed error correction capability}:
\begin{equation}
t_{\text{correct}} = \left\lfloor \frac{d_{\text{free}} - 1}{2} \right\rfloor
\label{eq:error-correction-capability}
\end{equation}

The code can \textbf{always correct} up to $t_{\text{correct}}$ errors. It can \textbf{detect} up to $d_{\text{free}} - 1$ errors.

\subsection{Common Convolutional Codes}

\begin{center}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Code} & \textbf{$K$} & \textbf{Rate} & \textbf{$d_{\text{free}}$} & \textbf{$t$} & \textbf{States} \\
\midrule
(5, 7) & 3 & 1/2 & 5 & 2 & 4 \\
(171, 133) & 7 & 1/2 & 10 & 4 & 64 \\
(561, 753) & 9 & 1/2 & 12 & 5 & 256 \\
(1167, 1375, 1545) & 9 & 1/3 & 18 & 8 & 256 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Observations:}
\begin{itemize}
\item \textbf{Larger $K$}: Higher $d_{\text{free}}$ $\rightarrow$ better error correction
\item \textbf{Lower rate}: Higher $d_{\text{free}}$ at same $K$ (more redundancy)
\item \textbf{Trade-off}: Larger $K$ $\rightarrow$ more states $\rightarrow$ higher decoder complexity
\end{itemize}

\begin{calloutbox}{Why NASA Chose $K=7$}
The NASA standard $(171, 133)$ with $K=7$ represents an optimal balance:
\begin{itemize}
\item \textbf{Performance:} $d_{\text{free}} = 10$ provides excellent error correction
\item \textbf{Complexity:} 64 states is manageable even in 1970s hardware
\item \textbf{Memory:} Only 6 bits of shift register required
\item \textbf{Proven:} Decades of successful deployment in deep-space missions
\end{itemize}

Moving to $K=9$ would double the number of states (256) for only a 20\% improvement in $d_{\text{free}}$ (10 $\to$ 12).
\end{calloutbox}

\section{Performance Analysis}

\subsection{Bit Error Rate (BER)}

For BPSK modulation over AWGN channel with hard-decision Viterbi decoding:
\begin{equation}
P_b \approx \sum_{d=d_{\text{free}}}^{\infty} \beta_d \cdot Q\left(\sqrt{2 d r \frac{E_b}{N_0}}\right)
\label{eq:ber-convolutional}
\end{equation}
where:
\begin{itemize}
\item $P_b$ = bit error probability
\item $\beta_d$ = number of information bit errors for paths at Hamming distance $d$
\item $r$ = code rate
\item $E_b/N_0$ = energy per information bit to noise spectral density ratio
\item $Q(x) = \frac{1}{\sqrt{2\pi}} \int_x^\infty e^{-t^2/2} dt$ = Gaussian Q-function
\end{itemize}

\textbf{High SNR approximation:} At large $E_b/N_0$, the BER is dominated by the $d_{\text{free}}$ term:
\begin{equation}
P_b \approx \beta_{d_{\text{free}}} \cdot Q\left(\sqrt{2 d_{\text{free}} r \frac{E_b}{N_0}}\right)
\label{eq:ber-high-snr}
\end{equation}

\subsection{Coding Gain}

The \textbf{asymptotic coding gain} (compared to uncoded BPSK) is:
\begin{equation}
G_c = 10 \log_{10}(r \cdot d_{\text{free}}) \quad \text{(dB)}
\label{eq:coding-gain}
\end{equation}

\textbf{Worked Example: NASA Code $(171, 133)$}

Given:
\begin{itemize}
\item $K = 7$, $r = 1/2$, $d_{\text{free}} = 10$
\end{itemize}

Coding gain:
\begin{equation}
G_c = 10 \log_{10}(0.5 \times 10) = 10 \log_{10}(5) = 6.99 \approx 7.0 \text{ dB}
\end{equation}

\textbf{With soft-decision decoding:} Add approximately 2--2.5 dB additional gain:
\begin{equation}
G_{c,\text{soft}} \approx 7.0 + 2.0 = 9.0 \text{ dB}
\end{equation}

This means at BER $= 10^{-5}$, the coded system requires 9 dB \textbf{less} $E_b/N_0$ than uncoded BPSK.

\subsection{BER Performance Comparison}

\begin{center}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{$E_b/N_0$ (dB)} & \textbf{Uncoded BPSK} & \textbf{Conv (hard)} & \textbf{Conv (soft)} \\
\midrule
2 & $2.4 \times 10^{-2}$ & $7 \times 10^{-3}$ & $2 \times 10^{-3}$ \\
4 & $1.2 \times 10^{-3}$ & $3 \times 10^{-4}$ & $5 \times 10^{-5}$ \\
6 & $2.4 \times 10^{-5}$ & $2 \times 10^{-6}$ & $1 \times 10^{-7}$ \\
8 & $1.9 \times 10^{-7}$ & $5 \times 10^{-9}$ & $5 \times 10^{-10}$ \\
\bottomrule
\end{tabular}
\end{center}

\begin{keyconcept}
\textbf{Soft-decision Viterbi decoding provides approximately 2--2.5 dB gain} over hard-decision at the same BER. This is achieved by utilizing the \textbf{reliability information} from the demodulator (analog samples) rather than making hard binary decisions.

For systems where every dB matters (satellite, deep-space), this ``free'' gain is critical.
\end{keyconcept}

\subsubsection{Example Performance (NASA
K=7)}\label{example-performance-nasa-k7}

{\def\LTcaptype{} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Eb/N0 (dB) & Uncoded BPSK & Conv (hard) & Conv (soft) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2 &
2.4\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{2\}
&
7\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{3\}
&
2\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{3\} \\
4 &
1.2\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{3\}
&
3\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{4\}
&
5\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{5\} \\
6 &
2.4\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{5\}
&
2\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{6\}
&
1\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{7\} \\
8 &
1.9\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{7\}
&
5\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{9\}
&
5\$\textbackslash times\$10\textbackslash textsuperscript\{-\}\textbackslash textsuperscript\{1\}\textbackslash textsuperscript\{0\} \\
\end{longtable}
}

\textbf{Soft-decision gain}: \textasciitilde2 dB at BER \(10^{-5}\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Puncturing}\label{puncturing}

\textbf{Puncturing}: Delete some output bits to \textbf{increase code
rate}

\textbf{Example}: r=1/2 \$\textbackslash rightarrow\$ r=2/3 (delete
every 3rd bit)

\textbf{Puncturing pattern}: Matrix specifying which bits to keep

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{Example: Rate 2/3 from Rate
1/2}\label{example-rate-23-from-rate-12}

\textbf{Original}: 1 input \$\textbackslash rightarrow\$ 2 outputs (Y1,
Y2)

\textbf{Punctured (2 periods)}:

{\def\LTcaptype{} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Period & Input & Y1 & Y2 & Transmitted \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & bit 1 & & & Y1, Y2 \\
2 & bit 2 & & & Y1 only \\
\end{longtable}
}

\textbf{Result}: 2 inputs \$\textbackslash rightarrow\$ 3 outputs (rate
2/3)

\textbf{Puncturing matrix}:

\[
P = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix}
\]

\textbf{1} = transmit, \textbf{0} = delete

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{Common Punctured Rates}\label{common-punctured-rates}

\textbf{From r=1/2 base code}:

{\def\LTcaptype{} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Target Rate & Puncturing Period & Complexity \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{2/3} & 2 & Low \\
\textbf{3/4} & 3 & Low \\
\textbf{4/5} & 4 & Low \\
\textbf{5/6} & 5 & Moderate \\
\textbf{7/8} & 7 & Moderate \\
\end{longtable}
}

\textbf{Used in}: WiFi (802.11a/g), LTE, DVB

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Tail-Biting}\label{tail-biting}

\textbf{Problem}: Standard encoding requires \textbf{flushing} (adds
\(K-1\) zero bits)

\textbf{Overhead}: \((K-1)/L\) for message length \(L\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{Tail-Biting Solution}\label{tail-biting-solution}

\textbf{Start encoder in non-zero state} such that ending state =
starting state

\textbf{Result}: No flush bits needed (circular encoding)

\textbf{Decoding}: Try all \(2^{K-1}\) starting states, pick best

\textbf{Benefit}: No overhead (useful for short packets)

\textbf{Used in}: LTE control channels

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Recursive Systematic Convolutional
(RSC)}\label{recursive-systematic-convolutional-rsc}

\textbf{Recursive}: Output fed back to input

\textbf{Systematic}: One output = input (uncoded)

\textbf{Structure}:

\begin{verbatim}
        +--------<---------+
        |                  |
Input ->+--[Encoder]--+----+--> Output (systematic)
                      |
                      +-------> Output (parity)
\end{verbatim}

\textbf{Advantage}: Better for \textbf{Turbo codes} (interleaver gain)

\textbf{Used in}: Turbo codes, LTE Turbo codes

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Practical Applications}\label{practical-applications}

\subsubsection{1. Deep Space (Voyager)}\label{deep-space-voyager}

\textbf{Code}: (171, 133), K=7, r=1/2

\textbf{Eb/N0}: \textasciitilde1 dB (extremely weak signal)

\textbf{BER}: \(5 \times 10^{-3}\) (after Viterbi)

\textbf{Outer code}: RS(255,223) corrects residual errors

\textbf{Final BER}: \textless{} \(10^{-10}\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{2. WiFi (802.11a/g)}\label{wifi-802.11ag}

\textbf{Base code}: K=7, r=1/2

\textbf{Punctured rates}: 1/2, 2/3, 3/4

\textbf{Combined with}: OFDM (64-QAM subcarriers)

\textbf{Example (54 Mbps mode)}: - 64-QAM (6 bits/symbol) - Rate 3/4
convolutional code - Effective: 4.5 bits/symbol/subcarrier

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{3. LTE (Before Turbo)}\label{lte-before-turbo}

\textbf{Early 3G}: Used convolutional codes

\textbf{Parameters}: K=9, r=1/3

\textbf{Puncturing}: Adaptive (1/2, 2/3, 3/4, 5/6) based on channel

\textbf{Replaced by}: Turbo codes in LTE (better performance)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{4. GPS L1 C/A}\label{gps-l1-ca}

\textbf{Code}: K=7, r=1/2 (similar to NASA standard)

\textbf{Navigation message}: 50 bps

\textbf{After encoding}: 100 sps

\textbf{Combined with}: BPSK, CDMA spreading (1.023 Mcps)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{5. DVB-S (Satellite TV)}\label{dvb-s-satellite-tv}

\textbf{Inner code}: K=7, r=1/2, punctured to 2/3, 3/4, 5/6, 7/8

\textbf{Outer code}: RS(204,188)

\textbf{Concatenation}: Convolutional handles random errors, RS handles
bursts

\textbf{Result}: Robust satellite link (rain fade, interference)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Viterbi Decoder
Implementation}\label{viterbi-decoder-implementation}

\subsubsection{Computational Complexity}\label{computational-complexity}

\textbf{Per time step}: - \(2^K\) branch metric computations -
\(2^{K-1}\) add-compare-select (ACS) operations

\textbf{Memory}: Store \(2^{K-1}\) survivor paths (length
\$\textbackslash approx\$ 5K)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{Traceback Depth}\label{traceback-depth}

\textbf{Typical}: \(5K\) to \(7K\) (5-7 times constraint length)

\textbf{Example}: K=7 \$\textbackslash rightarrow\$ Traceback 35-50
steps

\textbf{Trade-off}: Longer traceback \$\textbackslash rightarrow\$
Better decisions, more memory/latency

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{Fixed-Point vs
Floating-Point}\label{fixed-point-vs-floating-point}

\textbf{Fixed-point}: 6-8 bits sufficient for metrics (quantization)

\textbf{Benefit}: Faster, less power (embedded systems)

\textbf{Performance loss}: Negligible (\textless0.1 dB)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Python Example: Simple Viterbi
(K=3)}\label{python-example-simple-viterbi-k3}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ convolutional\_encode\_k3(data):}
    \CommentTok{"""Encode using K=3, r=1/2, g1=111, g2=101."""}
\NormalTok{    state }\OperatorTok{=} \DecValTok{0}  \CommentTok{\# Initial state (00)}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ []}
    
    \ControlFlowTok{for}\NormalTok{ bit }\KeywordTok{in}\NormalTok{ data:}
        \CommentTok{\# Update state}
\NormalTok{        state }\OperatorTok{=}\NormalTok{ ((state }\OperatorTok{\textless{}\textless{}} \DecValTok{1}\NormalTok{) }\OperatorTok{|}\NormalTok{ bit) }\OperatorTok{\&} \BaseNTok{0b11}  \CommentTok{\# Shift and mask to 2 bits}
        
        \CommentTok{\# Compute outputs (XOR of taps)}
        \CommentTok{\# g1 = 111 (all 3 positions)}
        \CommentTok{\# g2 = 101 (positions 0 and 2)}
\NormalTok{        y1 }\OperatorTok{=}\NormalTok{ (state }\OperatorTok{\textgreater{}\textgreater{}} \DecValTok{0}\NormalTok{) }\OperatorTok{\^{}}\NormalTok{ (state }\OperatorTok{\textgreater{}\textgreater{}} \DecValTok{1}\NormalTok{) }\OperatorTok{\^{}}\NormalTok{ (bit)  }\CommentTok{\# g1}
\NormalTok{        y2 }\OperatorTok{=}\NormalTok{ (state }\OperatorTok{\textgreater{}\textgreater{}} \DecValTok{0}\NormalTok{) }\OperatorTok{\^{}}\NormalTok{ (bit)  }\CommentTok{\# g2}
        
\NormalTok{        output.extend([y1 }\OperatorTok{\&} \DecValTok{1}\NormalTok{, y2 }\OperatorTok{\&} \DecValTok{1}\NormalTok{])}
    
    \CommentTok{\# Flush (add 2 zeros)}
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{2}\NormalTok{):}
\NormalTok{        state }\OperatorTok{=}\NormalTok{ (state }\OperatorTok{\textless{}\textless{}} \DecValTok{1}\NormalTok{) }\OperatorTok{\&} \BaseNTok{0b11}
\NormalTok{        y1 }\OperatorTok{=}\NormalTok{ (state }\OperatorTok{\textgreater{}\textgreater{}} \DecValTok{0}\NormalTok{) }\OperatorTok{\^{}}\NormalTok{ (state }\OperatorTok{\textgreater{}\textgreater{}} \DecValTok{1}\NormalTok{)}
\NormalTok{        y2 }\OperatorTok{=}\NormalTok{ (state }\OperatorTok{\textgreater{}\textgreater{}} \DecValTok{0}\NormalTok{)}
\NormalTok{        output.extend([y1 }\OperatorTok{\&} \DecValTok{1}\NormalTok{, y2 }\OperatorTok{\&} \DecValTok{1}\NormalTok{])}
    
    \ControlFlowTok{return}\NormalTok{ output}

\KeywordTok{def}\NormalTok{ viterbi\_decode\_k3(received):}
    \CommentTok{"""Viterbi decoding for K=3, r=1/2, g1=111, g2=101."""}
    \CommentTok{\# Trellis: 4 states (00, 01, 10, 11)}
    \CommentTok{\# Branch outputs (state, input) {-}\textgreater{} (next\_state, output)}
    
    \CommentTok{\# Precompute branch outputs}
    \KeywordTok{def}\NormalTok{ branch\_output(state, input\_bit):}
\NormalTok{        next\_state }\OperatorTok{=}\NormalTok{ ((state }\OperatorTok{\textless{}\textless{}} \DecValTok{1}\NormalTok{) }\OperatorTok{|}\NormalTok{ input\_bit) }\OperatorTok{\&} \BaseNTok{0b11}
\NormalTok{        y1 }\OperatorTok{=}\NormalTok{ (state }\OperatorTok{\textgreater{}\textgreater{}} \DecValTok{0}\NormalTok{) }\OperatorTok{\^{}}\NormalTok{ (state }\OperatorTok{\textgreater{}\textgreater{}} \DecValTok{1}\NormalTok{) }\OperatorTok{\^{}}\NormalTok{ input\_bit}
\NormalTok{        y2 }\OperatorTok{=}\NormalTok{ (state }\OperatorTok{\textgreater{}\textgreater{}} \DecValTok{0}\NormalTok{) }\OperatorTok{\^{}}\NormalTok{ input\_bit}
        \ControlFlowTok{return}\NormalTok{ next\_state, [y1 }\OperatorTok{\&} \DecValTok{1}\NormalTok{, y2 }\OperatorTok{\&} \DecValTok{1}\NormalTok{]}
    
\NormalTok{    num\_states }\OperatorTok{=} \DecValTok{4}
\NormalTok{    L }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(received) }\OperatorTok{//} \DecValTok{2}  \CommentTok{\# Number of time steps}
    
    \CommentTok{\# Initialize path metrics (PM)}
\NormalTok{    pm }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{float}\NormalTok{(}\StringTok{\textquotesingle{}inf\textquotesingle{}}\NormalTok{)] }\OperatorTok{*}\NormalTok{ num\_states}
\NormalTok{    pm[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{0}  \CommentTok{\# Start at state 00}
    
    \CommentTok{\# Survivor paths}
\NormalTok{    survivors }\OperatorTok{=}\NormalTok{ [[]]  }\OperatorTok{*}\NormalTok{ num\_states}
    
    \CommentTok{\# Process each time step}
    \ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(L):}
\NormalTok{        r }\OperatorTok{=}\NormalTok{ received[}\DecValTok{2}\OperatorTok{*}\NormalTok{t:}\DecValTok{2}\OperatorTok{*}\NormalTok{t}\OperatorTok{+}\DecValTok{2}\NormalTok{]  }\CommentTok{\# Received 2 bits}
        
\NormalTok{        new\_pm }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{float}\NormalTok{(}\StringTok{\textquotesingle{}inf\textquotesingle{}}\NormalTok{)] }\OperatorTok{*}\NormalTok{ num\_states}
\NormalTok{        new\_survivors }\OperatorTok{=}\NormalTok{ [}\VariableTok{None}\NormalTok{] }\OperatorTok{*}\NormalTok{ num\_states}
        
        \ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_states):}
            \ControlFlowTok{if}\NormalTok{ pm[s] }\OperatorTok{==} \BuiltInTok{float}\NormalTok{(}\StringTok{\textquotesingle{}inf\textquotesingle{}}\NormalTok{):}
                \ControlFlowTok{continue}
            
            \ControlFlowTok{for}\NormalTok{ input\_bit }\KeywordTok{in}\NormalTok{ [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]:}
\NormalTok{                next\_s, expected }\OperatorTok{=}\NormalTok{ branch\_output(s, input\_bit)}
                
                \CommentTok{\# Hamming distance (hard decision)}
\NormalTok{                metric }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(r[i] }\OperatorTok{!=}\NormalTok{ expected[i] }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{2}\NormalTok{))}
                
                \CommentTok{\# Update path metric}
\NormalTok{                candidate\_pm }\OperatorTok{=}\NormalTok{ pm[s] }\OperatorTok{+}\NormalTok{ metric}
                
                \ControlFlowTok{if}\NormalTok{ candidate\_pm }\OperatorTok{\textless{}}\NormalTok{ new\_pm[next\_s]:}
\NormalTok{                    new\_pm[next\_s] }\OperatorTok{=}\NormalTok{ candidate\_pm}
\NormalTok{                    new\_survivors[next\_s] }\OperatorTok{=}\NormalTok{ survivors[s] }\OperatorTok{+}\NormalTok{ [input\_bit]}
        
\NormalTok{        pm }\OperatorTok{=}\NormalTok{ new\_pm}
\NormalTok{        survivors }\OperatorTok{=}\NormalTok{ new\_survivors}
    
    \CommentTok{\# Find best final state (should be 00 after flushing)}
\NormalTok{    best\_state }\OperatorTok{=} \DecValTok{0}
\NormalTok{    best\_pm }\OperatorTok{=}\NormalTok{ pm[}\DecValTok{0}\NormalTok{]}
    
    \CommentTok{\# Traceback}
\NormalTok{    decoded }\OperatorTok{=}\NormalTok{ survivors[best\_state][:}\OperatorTok{{-}}\DecValTok{2}\NormalTok{]  }\CommentTok{\# Remove flush bits}
    \ControlFlowTok{return}\NormalTok{ decoded}

\CommentTok{\# Example usage}
\NormalTok{data }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Original data: }\SpecialCharTok{\{}\NormalTok{data}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\NormalTok{encoded }\OperatorTok{=}\NormalTok{ convolutional\_encode\_k3(data)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Encoded: }\SpecialCharTok{\{}\NormalTok{encoded}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Simulate error (flip 1 bit)}
\NormalTok{received }\OperatorTok{=}\NormalTok{ encoded.copy()}
\NormalTok{received[}\DecValTok{3}\NormalTok{] }\OperatorTok{\^{}=} \DecValTok{1}  \CommentTok{\# Flip bit 3}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Received (1 error): }\SpecialCharTok{\{}\NormalTok{received}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\NormalTok{decoded }\OperatorTok{=}\NormalTok{ viterbi\_decode\_k3(received)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Decoded: }\SpecialCharTok{\{}\NormalTok{decoded}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Match: }\SpecialCharTok{\{}\NormalTok{decoded }\OperatorTok{==}\NormalTok{ data}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Output}:

\begin{verbatim}
Original data: [1, 0, 1, 1, 0]
Encoded: [1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1]
Received (1 error): [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1]
Decoded: [1, 0, 1, 1, 0]
Match: True
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Comparison: Block vs
Convolutional}\label{comparison-block-vs-convolutional}

{\def\LTcaptype{} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Property & Block Codes & Convolutional Codes \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Encoding} & Fixed blocks & Continuous stream \\
\textbf{Memory} & None (memoryless) & Yes (shift register) \\
\textbf{Decoding} & Algebraic (syndrome) & Viterbi (trellis search) \\
\textbf{Latency} & Block delay & Traceback depth (\textasciitilde5K) \\
\textbf{Soft-decision} & Possible (LLRs) & Natural (Viterbi) \\
\textbf{Best use} & Burst errors (RS) & Random errors (AWGN) \\
\end{longtable}
}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Design Guidelines}\label{design-guidelines}

\textbf{Choose K}: - \textbf{K=3-5}: Low complexity, embedded systems -
\textbf{K=7}: Standard (NASA, WiFi), good performance - \textbf{K=9}:
Better performance, higher complexity

\textbf{Choose rate}: - \textbf{1/2}: Strong coding (deep space) -
\textbf{1/3}: Very strong (low SNR) - \textbf{2/3, 3/4}: High throughput
(punctured)

\textbf{Soft-decision}: Always use if demodulator provides LLRs (+2 dB
free gain!)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Related Topics}\label{related-topics}

\begin{itemize}
\tightlist
\item
  \textbf{{[}{[}Block-Codes-(Hamming,-BCH,-Reed-Solomon){]}{]}}:
  Alternative FEC approach
\item
  \textbf{{[}{[}Turbo-Codes{]}{]}}: Concatenated convolutional codes
  (next-gen)
\item
  \textbf{{[}{[}LDPC-Codes{]}{]}}: Modern capacity-approaching codes
\item
  \textbf{{[}{[}Forward-Error-Correction-(FEC){]}{]}}: General FEC
  overview
\item
  \textbf{{[}{[}Bit-Error-Rate-(BER){]}{]}}: Performance metric
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Key takeaway}: \textbf{Convolutional codes use memory (shift
register + XOR) for continuous encoding.} Constraint length \(K\)
determines states (\(2^{K-1}\)) and performance (\(d_{\text{free}}\)
increases with \(K\)). Viterbi algorithm performs optimal ML decoding
via trellis search. Soft-decision Viterbi gains \textasciitilde2 dB over
hard-decision. Puncturing increases code rate (1/2
\$\textbackslash rightarrow\$ 2/3, 3/4). NASA standard (171, 133) K=7,
\(d_{\text{free}}=10\), \textasciitilde7 dB coding gain. Used in
Voyager, GPS, WiFi, DVB. Turbo codes (parallel concatenated
convolutional) achieve near-Shannon performance. Trade-off: Larger \(K\)
= better correction but higher complexity.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\emph{This wiki is part of the {[}{[}Home\textbar Chimera Project{]}{]}
documentation.}
